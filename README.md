# AI Agents Simulation

A 3D interactive simulation featuring AI agents that can move, observe, communicate, and make autonomous decisions in a virtual world. The simulation combines a Flask backend with a Babylon.js frontend to create an immersive multi-agent environment.

## ğŸ¯ Overview

This project simulates intelligent agents (Alice and Bob) in a 3D world where they can:
- **Move autonomously** around the environment
- **Observe** other agents and world objects
- **Communicate** with each other and users
- **Make decisions** based on their personality and observations
- **Remember** past interactions and experiences

## ğŸ—ï¸ Architecture

### Backend (Flask)
- **Framework**: Flask with CORS support
- **Port**: 5001
- **Architecture**: Service-oriented with clear separation of concerns

### Frontend (Babylon.js)
- **3D Engine**: Babylon.js for 3D rendering
- **Architecture**: Modular JavaScript classes
- **UI**: Interactive 3D scene with chat interface

## ğŸ”§ Backend Components

### Core Services

#### 1. **Agent Service** (`services/agent_service.py`)
- **Purpose**: Manages AI agents and their lifecycle
- **Key Methods**:
  - `get_all_agents()`: Returns all configured agents
  - `start_simulation()`: Starts the AI decision loop
  - `force_agent_decision()`: Forces immediate agent decision
  - `reset_simulation()`: Resets all agents to initial state

#### 2. **Brain Coordination Service** (`services/brain_coordination_service.py`)
- **Purpose**: Manages AI agent brains and coordinates their interactions
- **Key Features**:
  - **Brain Management**: Manages agent brains and their states
  - **Memory Coordination**: Tracks agent memories and learning
  - **World Reference**: Provides world objects that brains can reference

#### 3. **Agent Model** (`models/agent.py`)
- **Purpose**: Represents individual AI agents with personality and behavior
- **Key Properties**:
  - `position`: 3D coordinates (X-Z plane movement, Y is height)
  - `personality`: Creative/artistic vs Logical/analytical
  - `memory`: Recent experiences and observations
  - `current_action`: Current behavior (idle, move, say)
  - `observations`: Real-time environmental awareness

### Backend States

#### Agent States
1. **Idle**: Agent is waiting for next decision
2. **Moving**: Agent is traveling to a target position
3. **Speaking**: Agent is communicating (3-second duration)
4. **Observing**: Agent is analyzing the environment

#### Brain States
1. **Inactive**: Brains not coordinating
2. **Active**: Brains coordinating and managing agent interactions
3. **Learning**: Brains processing memories and experiences

### API Endpoints

#### Agent Management
- `GET /api/agents` - Get all agents
- `POST /api/agents/{id}/chat` - Chat with specific agent
- `GET /api/agents/{id}/conversation` - Get conversation history
- `POST /api/agents/{id}/reset` - Reset agent conversation

#### Brain Coordination
- `POST /api/agents/brain/decide` - Request decision from agent brain
- `POST /api/agents/brain/action-complete` - Report action completion to brain
- `GET /api/agents/brain/state` - Get brain coordination state

#### Agent Actions
- `POST /api/agents/{id}/force-decision` - Force immediate decision
- `POST /api/agents/{id}/report-movement` - Report movement completion
- `POST /api/agents/{id}/clear-pending-decision` - Clear pending decision

## ğŸ® Frontend Components

### Modular Architecture

The frontend is organized into focused, maintainable modules:

```
static/js/
â”œâ”€â”€ modules/           # Core application modules
â”‚   â”œâ”€â”€ agent-manager.js      # Agent management and 3D visualization
â”‚   â””â”€â”€ world-simulator.js    # Main coordinator and simulation logic
â”œâ”€â”€ systems/           # Game/simulation systems
â”‚   â”œâ”€â”€ movement-system.js    # Movement animations and physics
â”‚   â””â”€â”€ sensory-system.js     # Sensory data collection and processing
â”œâ”€â”€ services/          # External service integrations
â”‚   â””â”€â”€ brain-service.js      # Backend AI brain communication
â”œâ”€â”€ app.js            # Main application entry point
â”œâ”€â”€ chat.js           # Chat functionality
â”œâ”€â”€ chatbubbles.js    # 3D chat bubble rendering
â””â”€â”€ scene.js          # Babylon.js scene management
```

### Core Modules

#### 1. **Scene Manager** (`static/js/scene.js`)
- **Purpose**: Manages 3D scene, camera, lighting, and rendering
- **Features**:
  - ArcRotateCamera with mouse controls
  - Hemispheric and directional lighting
  - Ground plane and world objects
  - Responsive rendering loop

#### 2. **Agent Manager** (`static/js/modules/agent-manager.js`)
- **Purpose**: Manages individual agents and their 3D representation
- **Key Features**:
  - **Agent Creation**: Initialize agent bodies and properties
  - **3D Capsules**: Visual representation of agents
  - **State Management**: Track agent actions and properties
  - **Click Interaction**: Agent selection for chatting
  - **Visual Updates**: Position and appearance synchronization

#### 3. **World Simulator** (`static/js/modules/world-simulator.js`)
- **Purpose**: Main coordinator and world state management
- **Key Features**:
  - **Event-Driven Simulation**: Triggers decisions based on events, not timers
  - **Decision Coordination**: Request decisions from agent brains
  - **Action Execution**: Coordinate movement, speech, and observation
  - **Anti-Stuck Monitoring**: Periodic checks for idle agents near interesting things
  - **System Integration**: Orchestrates all other modules
  - **Unified Decision Triggering**: Single method handles all decision triggers

#### 4. **Movement System** (`static/js/systems/movement-system.js`)
- **Purpose**: Handles movement animations and physics
- **Key Features**:
  - **Smooth Animation**: Interpolation between positions
  - **Distance Calculation**: Movement duration and completion
  - **Physics**: Realistic movement behavior
  - **Completion Reporting**: Notify brains when movement finishes

#### 5. **Sensory System** (`static/js/systems/sensory-system.js`)
- **Purpose**: Handles sensory data collection and processing
- **Key Features**:
  - **Change Detection**: Tracks changes in nearby agents and objects
  - **Event Triggers**: Automatically triggers decisions when interesting things appear/disappear
  - **Nearby Detection**: Find agents within observation radius (1 unit)
  - **Object Visibility**: Detect world objects (coins, landmarks) in range
  - **Sensory Data**: Compile comprehensive environmental data
  - **World Objects**: Manage interactive world elements

#### 6. **Brain Service** (`static/js/services/brain-service.js`)
- **Purpose**: Communication with AI brains on the backend
- **Key Features**:
  - **Decision Requests**: Ask brains for agent decisions
  - **Action Reporting**: Notify brains of completed actions
  - **API Communication**: Handle all backend interactions
  - **Error Handling**: Robust communication with fallbacks

#### 7. **Chat Manager** (`static/js/chat.js`)
- **Purpose**: Manages user-agent communication
- **Features**:
  - Interactive chat panel
  - Conversation history
  - Real-time message exchange
  - Agent personality display

#### 8. **Main App** (`static/js/app.js`)
- **Purpose**: Orchestrates all components
- **Features**:
  - Application initialization
  - Simulation controls
  - Error handling
  - Event coordination

### Frontend States

#### Application States
1. **Initializing**: Loading 3D scene and agents
2. **Ready**: All components loaded, ready for interaction
3. **Error**: Initialization failed, showing error overlay

#### Simulation States
1. **Stopped**: No simulation running, agents idle
2. **Running**: Simulation active, agents making decisions
3. **Updating**: Processing agent state changes (50ms intervals)

#### Agent Visual States
1. **Idle**: Agent capsule stationary
2. **Moving**: Agent capsule animating to target
3. **Speaking**: Chat bubble visible with message
4. **Selected**: Agent highlighted for chat interaction

## ğŸ¤– Agent Behavior System

### Event-Driven Decision Making

The simulation now uses an **event-driven decision system** instead of time-based intervals, making agents much more responsive and natural:

#### **Decision Triggers**
1. **Action Completion Events**:
   - When agents finish moving to a target
   - When agents complete speaking (after 3 seconds)
   - When agents finish observing their environment
   - When agents collect coins

2. **Interesting Observation Events**:
   - When agents see new agents enter their observation radius
   - When agents see new world objects (coins, landmarks) enter their range
   - When agents lose sight of previously visible agents or objects

3. **Initialization Events**:
   - When simulation starts, all agents immediately make their first decision
   - No more waiting for the first timer tick

#### **Decision Making Process**

1. **Observation Phase**:
   - Agents scan for other agents within `observation_radius` (1 unit)
   - Detect world objects (coins, landmarks) within visibility range
   - Track changes in their environment in real-time

2. **Memory Integration**:
   - Recent memories influence decisions
   - Past interactions shape behavior
   - Maximum 10 memory items per agent

3. **Decision Generation**:
   - **Move**: Travel to interesting locations or other agents
   - **Speak**: Communicate when near other agents (50% chance when close)
   - **Observe**: Analyze environment when idle
   - **Idle**: Wait and think (reduced to 5% chance overall)

4. **Anti-Stuck Mechanisms**:
   - Agents near interesting things are more likely to interact (80% chance to move when near other agents)
   - Periodic checks every 500ms for idle agents near interesting things
   - Fallback system forces decisions if agents are idle for more than 5 seconds

### Personality-Based Behavior

#### Alice (Creative & Artistic)
- **Colors**: Red capsule
- **Behavior**: Enthusiastic, creative responses
- **Movement**: Drawn to interesting objects
- **Communication**: Inspirational and artistic language

#### Bob (Logical & Analytical)
- **Colors**: Blue capsule
- **Behavior**: Methodical, analytical responses
- **Movement**: Systematic exploration
- **Communication**: Precise and logical language

## ğŸŒ World Environment

### 3D Coordinate System
- **X-Z Plane**: Horizontal movement (agents move on this plane)
- **Y Axis**: Height (fixed for agents at 0.6 units)
- **Origin**: Center of the world

### World Objects
- **Purple Sphere**: Landmark at (5, 0.5, 3) that agents can investigate
- **Collectible Coins**: 10 golden coins scattered around the world for agents to collect
- **Ground Plane**: 10x10 unit gray surface
- **Lighting**: Hemispheric + directional lighting for visibility

## ğŸ® User Controls

### 3D Navigation
- **Mouse**: Look around the scene
- **WASD**: Move camera forward/back/left/right
- **Q/E**: Move camera up/down
- **Mouse Wheel**: Zoom in/out
- **Click Agents**: Open chat interface

### Simulation Controls
- **Start Simulation**: Begin AI decision-making
- **Stop Simulation**: Pause agent autonomy
- **Reset**: Return agents to starting positions
- **Force Decisions**: Make agents decide immediately

## ğŸ“ Project Structure

```
simulation/
â”œâ”€â”€ app.py                    # Flask application entry point
â”œâ”€â”€ config.py                 # Configuration settings
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ models/                   # Data models
â”‚   â”œâ”€â”€ agent.py             # Agent class with AI decision logic
â”‚   â””â”€â”€ conversation.py      # Conversation management
â”œâ”€â”€ services/                 # Backend services
â”‚   â”œâ”€â”€ agent_service.py     # Agent management service
â”‚   â”œâ”€â”€ brain_coordination_service.py  # AI brain coordination
â”‚   â””â”€â”€ conversation_service.py        # Chat management
â”œâ”€â”€ routes/                   # API endpoints
â”‚   â”œâ”€â”€ agent_routes.py      # Agent-related endpoints
â”‚   â””â”€â”€ main_routes.py       # Main application routes
â”œâ”€â”€ static/js/               # Frontend JavaScript modules
â”‚   â”œâ”€â”€ modules/             # Core application modules
â”‚   â”‚   â”œâ”€â”€ agent-manager.js
â”‚   â”‚   â””â”€â”€ world-simulator.js
â”‚   â”œâ”€â”€ systems/             # Game/simulation systems
â”‚   â”‚   â”œâ”€â”€ movement-system.js
â”‚   â”‚   â””â”€â”€ sensory-system.js
â”‚   â”œâ”€â”€ services/            # External service integrations
â”‚   â”‚   â””â”€â”€ brain-service.js
â”‚   â”œâ”€â”€ app.js              # Main application
â”‚   â”œâ”€â”€ chat.js             # Chat functionality
â”‚   â”œâ”€â”€ chatbubbles.js      # 3D chat bubbles
â”‚   â””â”€â”€ scene.js            # 3D scene management
â”œâ”€â”€ templates/               # HTML templates
â”‚   â””â”€â”€ index.html          # Main application page
â”œâ”€â”€ static/css/             # Stylesheets
â”‚   â””â”€â”€ main.css           # Application styling
â””â”€â”€ logs/                   # Application logs
    â””â”€â”€ ai_agents_*.log    # Log files
```

## ğŸš€ Getting Started

### Prerequisites
- Python 3.7+
- Modern web browser with WebGL support

### Installation
   ```bash
# Install Python dependencies
   pip install -r requirements.txt

# Run the Flask server
   python app.py
   ```

### Access
- Open browser to `http://localhost:5001`
- Wait for 3D scene to load
- Click on agent capsules to start chatting
- Use simulation controls to start autonomous behavior

## ğŸ”„ Data Flow

### Modular Data Flow

#### 1. **Initialization Flow**
```
app.js â†’ world-simulator.js â†’ agent-manager.js â†’ scene.js
```

#### 2. **Event-Driven Simulation Flow**
```
Event Triggers:
â”œâ”€â”€ Action Completion â†’ world-simulator.js â†’ brain-service.js â†’ backend
â”œâ”€â”€ Observation Changes â†’ sensory-system.js â†’ world-simulator.js â†’ brain-service.js â†’ backend
â”œâ”€â”€ Movement Execution â†’ movement-system.js â†’ agent-manager.js â†’ scene.js
â””â”€â”€ Periodic Checks â†’ world-simulator.js â†’ checkIdleAgents() â†’ brain-service.js â†’ backend
```

#### 3. **User Interaction Flow**
```
user click â†’ agent-manager.js â†’ chat.js â†’ brain-service.js â†’ backend
```

### Backend â†’ Frontend
1. **Decision Results**: AI brain decisions sent to world-simulator
2. **Agent States**: Position, action, and utterance updates
3. **Chat Responses**: Agent replies to user messages

### Frontend â†’ Backend
1. **Sensory Data**: Environmental information sent to brains
2. **Action Completion**: Movement, speech, and observation results
3. **User Input**: Chat messages and simulation controls

## ğŸ¯ Key Features

### Real-time Interaction
- **50ms Update Loop**: Smooth agent movement and state updates
- **Event-Driven Decisions**: Agents respond immediately to interesting events
- **500ms Observation Checks**: Periodic monitoring for environmental changes
- **3s Speech Duration**: Realistic communication timing
- **0.1s Decision Interval**: Very responsive decision-making (down from 1.0s)

### Memory System
- **Short-term Memory**: Recent observations and decisions
- **Personality Persistence**: Consistent behavior patterns
- **Learning**: Agents remember past interactions

### Visual Feedback
- **Floating Chat Bubbles**: Real-time speech visualization
- **Smooth Animations**: Fluid movement between positions
- **Interactive UI**: Click-to-chat with any agent

## ğŸ› ï¸ Technical Details

### Backend Technologies
- **Flask**: Web framework
- **Flask-CORS**: Cross-origin resource sharing
- **Threading**: Background AI decision processing
- **JSON**: API communication

### Frontend Technologies
- **Babylon.js**: 3D graphics engine
- **Modular JavaScript**: Organized, maintainable code structure
- **CSS3**: Styling and animations
- **WebGL**: Hardware-accelerated rendering

### Modular Architecture Benefits
- **Maintainability**: Single responsibility principle, easy to debug
- **Scalability**: Easy to add new features without affecting existing code
- **Reusability**: Modules can be used independently in other projects
- **Team Development**: Multiple developers can work on different modules
- **Testing**: Each module can be unit tested independently

### Performance Considerations
- **Efficient Rendering**: 60 FPS target with requestAnimationFrame
- **Memory Management**: Automatic cleanup of disposed objects
- **Network Optimization**: Minimal API calls, efficient state updates
- **Modular Loading**: Only load necessary modules for better performance
- **Event-Driven Architecture**: Reduces unnecessary processing and improves responsiveness

## ğŸ§¹ Codebase Improvements

### Recent Optimizations
The codebase has been significantly cleaned up and optimized:

#### **Event-Driven System**
- **Replaced timer-based decisions** (every 2 seconds) with event-driven triggers
- **Immediate response** to interesting observations and action completions
- **Reduced decision interval** from 1.0s to 0.1s for faster responsiveness

#### **Code Consolidation**
- **Merged redundant methods**: Combined similar decision tracking functions
- **Unified decision triggering**: Single method handles all decision triggers
- **Eliminated duplicate code**: Removed ~100 lines of redundant code
- **Centralized helper methods**: Reduced repetitive `window.app.worldSimulator` checks

#### **Anti-Stuck Mechanisms**
- **Initial decision triggers**: Agents start immediately when simulation begins
- **Idle agent monitoring**: Periodic checks for agents stuck near interesting things
- **Fallback system**: Forces decisions if agents are idle for more than 5 seconds
- **Improved decision logic**: Reduced idle chance from 10% to 5% overall

#### **Better Architecture**
- **Clear separation of concerns**: Each system handles its own responsibilities
- **Simplified method signatures**: Reduced parameter passing and complexity
- **Improved maintainability**: Single methods handle similar functionality
- **Enhanced debugging**: Centralized decision triggering for easier tracing

## ğŸ”® Future Enhancements

- **Real AI Integration**: Replace mock decisions with actual LLM calls
- **More Agent Types**: Additional personalities and behaviors
- **Complex World**: More interactive objects and environments
- **Multi-user Support**: Multiple users observing the same simulation
- **Advanced Memory**: Long-term memory and learning systems
- **Custom Scenarios**: User-defined agent goals and challenges

---

*This simulation demonstrates the potential for AI agents to exist in shared virtual spaces, making autonomous decisions while remaining interactive and engaging for human users.*